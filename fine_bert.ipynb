{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b597b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load\n",
    "data = pd.read_csv(\"resources/cn7050data.csv\",encoding='latin-1',names=[\"sentiment\",\"text\"])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a32b023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment    0\n",
      "text         0\n",
      "dtype: int64\n",
      "sentiment\n",
      "neutral     2879\n",
      "positive    1363\n",
      "negative     604\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 4846 entries, 0 to 4845\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   sentiment  4846 non-null   str  \n",
      " 1   text       4846 non-null   str  \n",
      "dtypes: str(2)\n",
      "memory usage: 717.4 KB\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.isnull().sum())\n",
    "print(data.value_counts(\"sentiment\"))\n",
    "sns.countplot(x=\"sentiment\",data=data)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33ba4040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral     2879\n",
       "positive    1363\n",
       "negative     604\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts(\"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56b201",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfa042d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before removing duplicates: (4846, 2)\n",
      "     sentiment                                               text\n",
      "1098   neutral  The issuer is solely responsible for the conte...\n",
      "1099   neutral  The issuer is solely responsible for the conte...\n",
      "1415   neutral  The report profiles 614 companies including ma...\n",
      "1416   neutral  The report profiles 614 companies including ma...\n",
      "2395   neutral  Ahlstrom 's share is quoted on the NASDAQ OMX ...\n",
      "2396   neutral  Ahlstrom 's share is quoted on the NASDAQ OMX ...\n",
      "2566   neutral  SSH Communications Security Corporation is hea...\n",
      "2567   neutral  SSH Communications Security Corporation is hea...\n",
      "3093   neutral  Proha Plc ( Euronext :7327 ) announced today (...\n",
      "3094   neutral  Proha Plc ( Euronext :7327 ) announced today (...\n",
      "3205   neutral  The company serves customers in various indust...\n",
      "3206   neutral  The company serves customers in various indust...\n",
      "after removing duplicates: (4840, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentiment, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"before removing duplicates:\", data.shape)\n",
    "print(data[data.duplicated(subset=[\"sentiment\",\"text\"],keep=False)])\n",
    "data = data.drop_duplicates()\n",
    "data=data.reset_index(drop=True)\n",
    "print(\"after removing duplicates:\", data.shape)\n",
    "data[data.duplicated(subset=[\"sentiment\",\"text\"],keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "917a0f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all the text. probabely not needed but just to be sure\n",
    "data['sentiment'] = data['sentiment'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c82392",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "**'stratify'** sampling to ensure each split has the same proportion of positive/negative/neutral labels.\n",
    "neutral     2879\n",
    "positive    1363\n",
    "negative     604\n",
    "\n",
    "we are going to use an 80% Train, 10% Validation, 10% Test split.\n",
    "\n",
    "named: train / val/ test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51e6bb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training : 3872 rows\n",
      "Validation : 484 rows\n",
      "Testing : 484 rows\n"
     ]
    }
   ],
   "source": [
    "train_df,temp_df = train_test_split(data, test_size=0.2, random_state=13,stratify=data[\"sentiment\"])\n",
    "val_df,test_df = train_test_split(temp_df, test_size=0.5, random_state=13,stratify=temp_df[\"sentiment\"])\n",
    "\n",
    "print(f\"Training : {len(train_df)} rows\")\n",
    "print(f\"Validation : {len(val_df)} rows\")\n",
    "print(f\"Testing : {len(test_df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e298db40",
   "metadata": {},
   "source": [
    "## 1- FinBERT Baseline\n",
    "We will use the hugging face transformers libraries to call :\n",
    "\n",
    "1- ProsusAI/finbert\n",
    "\n",
    "2- bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66f228c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score,f1_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# I wanted to use the pretrained model on the train data but we probabely do not need it because we will compare the test dataset.\n",
    "# train_text=train_df[\"text\"].tolist()\n",
    "# train_labels=train_df[\"sentiment\"].tolist()\n",
    "\n",
    "# test data\n",
    "test_texts=test_df[\"text\"].tolist()\n",
    "test_labels=test_df[\"sentiment\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f7c2171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /ProsusAI/finbert/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x17ea1fad0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\"), '(Request ID: 790b81ef-1475-4123-9924-aed0511097a4)')' thrown while requesting HEAD https://huggingface.co/ProsusAI/finbert/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "/Users/Mehr/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running FineBert predictions...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.93      0.88        61\n",
      "     neutral       0.95      0.88      0.92       287\n",
      "    positive       0.81      0.90      0.85       136\n",
      "\n",
      "    accuracy                           0.89       484\n",
      "   macro avg       0.86      0.90      0.88       484\n",
      "weighted avg       0.90      0.89      0.89       484\n",
      "\n",
      "accuracy: 0.8925619834710744\n",
      "f1 score: 0.8938091178793657\n",
      "confusion matrix:\n",
      "[[ 57   1   3]\n",
      " [  9 253  25]\n",
      " [  3  11 122]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ingen is an established medical device manufac...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.5894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Russian officials inspected the damage and gav...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.7964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A state program has been worked out , legal pr...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.7616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr Ashley , deputy executive chairman of Sport...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.9475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EBIT excluding non-recurring items , totalled ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text true_label pred_label  \\\n",
       "0  Ingen is an established medical device manufac...    neutral   positive   \n",
       "1  Russian officials inspected the damage and gav...    neutral    neutral   \n",
       "2  A state program has been worked out , legal pr...    neutral    neutral   \n",
       "3  Mr Ashley , deputy executive chairman of Sport...    neutral    neutral   \n",
       "4  EBIT excluding non-recurring items , totalled ...   positive   positive   \n",
       "\n",
       "    score  \n",
       "0  0.5894  \n",
       "1  0.7964  \n",
       "2  0.7616  \n",
       "3  0.9475  \n",
       "4  0.9526  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finbert_classifier=pipeline(\"text-classification\", model=\"ProsusAI/finbert\",device=-1)\n",
    "\n",
    "print(\"Running FineBert predictions...\")\n",
    "# model=finbert_classifier(test_texts, truncation=True, max_length=512) # This line is redundant as we do it in the loop/variable below\n",
    "\n",
    "# Get both labels and scores\n",
    "results = finbert_classifier(test_texts, truncation=True, max_length=512)\n",
    "finbert_preds_labels = [result['label'] for result in results]\n",
    "finbert_preds_scores = [result['score'] for result in results] \n",
    "\n",
    "finebert_res=pd.DataFrame({\"text\":test_texts,\"true_label\":test_labels,\"pred_label\":finbert_preds_labels})\n",
    "\n",
    "#we use the result of the model trained on he test dataset to see how succesful it is to predict the sentiment.\n",
    "# the metrics:\n",
    "\n",
    "print(classification_report(finebert_res['true_label'], finebert_res['pred_label']))\n",
    "print(f\"accuracy: {accuracy_score(finebert_res['true_label'], finebert_res['pred_label'])}\")\n",
    "print(f\"f1 score: {f1_score(finebert_res['true_label'], finebert_res['pred_label'],average='weighted')}\")\n",
    "print(f\"confusion matrix:\\n{confusion_matrix(finebert_res['true_label'], finebert_res['pred_label'])}\")\n",
    "\n",
    "# Dataframe with the results\n",
    "finbert_res_df=pd.DataFrame({\n",
    "    \"text\":test_texts,\n",
    "    \"true_label\":test_labels, \n",
    "    \"pred_label\":finbert_preds_labels,\n",
    "    \"score\": [round(s, 4) for s in finbert_preds_scores]\n",
    "})\n",
    "\n",
    "finbert_res_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a2121",
   "metadata": {},
   "source": [
    "We see the model **accuracy** is 89% on test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489cf4db",
   "metadata": {},
   "source": [
    "## 2- BERT Baseline\n",
    "\n",
    "Now we load **bert-base-uncased** for sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f65c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/Mehr/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bert-uncased predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3849f5c2634e42af922c359afc8536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/484 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4401\n",
      "Macro F1: 0.2187\n",
      "Per-Class Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.02      0.03      0.02        61\n",
      "     neutral       0.55      0.74      0.63       287\n",
      "    positive       0.00      0.00      0.00       136\n",
      "\n",
      "    accuracy                           0.44       484\n",
      "   macro avg       0.19      0.26      0.22       484\n",
      "weighted avg       0.33      0.44      0.38       484\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ingen is an established medical device manufac...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.5235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Russian officials inspected the damage and gav...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.5117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A state program has been worked out , legal pr...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>0.5297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr Ashley , deputy executive chairman of Sport...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>0.5374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EBIT excluding non-recurring items , totalled ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>0.5477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text true_label pred_label  \\\n",
       "0  Ingen is an established medical device manufac...    neutral    LABEL_0   \n",
       "1  Russian officials inspected the damage and gav...    neutral    LABEL_0   \n",
       "2  A state program has been worked out , legal pr...    neutral    LABEL_1   \n",
       "3  Mr Ashley , deputy executive chairman of Sport...    neutral    LABEL_1   \n",
       "4  EBIT excluding non-recurring items , totalled ...   positive    LABEL_1   \n",
       "\n",
       "    score  \n",
       "0  0.5235  \n",
       "1  0.5117  \n",
       "2  0.5297  \n",
       "3  0.5374  \n",
       "4  0.5477  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_uncased=pipeline(\"text-classification\", model=\"bert-base-uncased\",device=-1)\n",
    "\n",
    "print(\"Running bert-uncased predictions...\")\n",
    "bert_preds_raw = [result['label'] for result in tqdm(bert_uncased(test_texts, truncation=True, max_length=512))]\n",
    "\n",
    "bert_uncased_results=bert_uncased(test_texts, truncation=True, max_length=512)\n",
    "\n",
    "label_mapping = {'LABEL_0': 'negative', 'LABEL_1': 'neutral', 'LABEL_2': 'positive'}\n",
    "bert_preds = [label_mapping[pred] for pred in bert_preds_raw]\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(test_labels, bert_preds):.4f}\")\n",
    "print(f\"Macro F1: {f1_score(test_labels, bert_preds, average='macro'):.4f}\")\n",
    "print(\"Per-Class Metrics:\")\n",
    "print(classification_report(test_labels, bert_preds, zero_division=0))\n",
    "\n",
    "\n",
    "bert_uncased_preds_labels = [result['label'] for result in bert_uncased_results]\n",
    "bert_uncased_preds_scores = [result['score'] for result in bert_uncased_results] \n",
    "\n",
    "bert_uncased_res_df=pd.DataFrame({\n",
    "    \"text\":test_texts,\n",
    "    \"true_label\":test_labels, \n",
    "    \"pred_label\":bert_uncased_preds_labels,\n",
    "    \"score\": [round(s, 4) for s in bert_uncased_preds_scores]\n",
    "})\n",
    "\n",
    "bert_uncased_res_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c440f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score (Weighted)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ProsusAI/finbert</td>\n",
       "      <td>0.892562</td>\n",
       "      <td>0.893809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert-base-uncased</td>\n",
       "      <td>0.440083</td>\n",
       "      <td>0.377678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model  Accuracy  F1 Score (Weighted)\n",
       "0   ProsusAI/finbert  0.892562             0.893809\n",
       "1  bert-base-uncased  0.440083             0.377678"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Collect metrics for comparison\n",
    "model_metrics = []\n",
    "\n",
    "# FinBERT Metrics\n",
    "finbert_acc = accuracy_score(test_labels, finbert_preds_labels)\n",
    "finbert_f1 = f1_score(test_labels, finbert_preds_labels, average='weighted')\n",
    "model_metrics.append({\n",
    "    'Model': 'ProsusAI/finbert',\n",
    "    'Accuracy': finbert_acc,\n",
    "    'F1 Score (Weighted)': finbert_f1\n",
    "})\n",
    "\n",
    "# BERT Uncased Metrics\n",
    "bert_acc = accuracy_score(test_labels, bert_preds)\n",
    "bert_f1 = f1_score(test_labels, bert_preds, average='weighted')\n",
    "model_metrics.append({\n",
    "    'Model': 'bert-base-uncased',\n",
    "    'Accuracy': bert_acc,\n",
    "    'F1 Score (Weighted)': bert_f1\n",
    "})\n",
    "\n",
    "#  comparison DataFrame\n",
    "comparison_df = pd.DataFrame(model_metrics)\n",
    "display(comparison_df)\n",
    "\n",
    "#  grouped bar chart using Pandas\n",
    "comparison_df.set_index('Model').plot(kind='bar', figsize=(11, 5))\n",
    "\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=0) # Keep labels horizontal\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f2c12",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "We can see the **Finbert** predicts much better than the **bert-uncased** model. This comparison is before fine tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb0f2a",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce714a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97930e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/Loading Tokenizer for bert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Mehr/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/Loading Tokenizer for ProsusAI/finbert...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a3dccce5d44900968f273267f649f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bc354f6ecc4576953f91fdf800f9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e576fedc8b54822971df954c629b54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e635a2f29649cca1b1221043d746b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m tokenized_val_bert = val_dataset.map(tokenize_function_bert, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     38\u001b[39m tokenized_test_bert = test_dataset.map(tokenize_function_bert, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m tokenized_train_finbert = \u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function_finbert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m tokenized_val_finbert = val_dataset.map(tokenize_function_finbert, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     42\u001b[39m tokenized_test_finbert = test_dataset.map(tokenize_function_finbert, batched=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3406\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3404\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3405\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3406\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3409\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3759\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3757\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3758\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3759\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3761\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3712\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3710\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3711\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3712\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3635\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3633\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3634\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3635\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3636\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtokenize_function_finbert\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize_function_finbert\u001b[39m(examples):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_finbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3055\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3053\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   3054\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m3055\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3056\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3057\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3142\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3137\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3138\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not match batch length of `text_pair`:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3139\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3140\u001b[39m         )\n\u001b[32m   3141\u001b[39m     batch_text_or_text_pairs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[32m-> \u001b[39m\u001b[32m3142\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3144\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3160\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3161\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_plus(\n\u001b[32m   3164\u001b[39m         text=text,\n\u001b[32m   3165\u001b[39m         text_pair=text_pair,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3182\u001b[39m         **kwargs,\n\u001b[32m   3183\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3338\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   3328\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3329\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3330\u001b[39m     padding=padding,\n\u001b[32m   3331\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3335\u001b[39m     **kwargs,\n\u001b[32m   3336\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3340\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3355\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3357\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:528\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001b[32m    526\u001b[39m     \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[32m    535\u001b[39m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[32m    536\u001b[39m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[32m    538\u001b[39m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[32m    540\u001b[39m tokens_and_encodings = [\n\u001b[32m    541\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_encoding(\n\u001b[32m    542\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    552\u001b[39m ]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "label2id = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "id2label = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "train_df['label'] = train_df['sentiment'].map(label2id)\n",
    "test_df['label'] = test_df['sentiment'].map(label2id)\n",
    "val_df['label'] = val_df['sentiment'].map(label2id)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['text', 'label']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['text', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n",
    "\n",
    "MODEL_NAME_BERT= \"bert-base-uncased\"\n",
    "MODEL_NAME_FINBERT= \"ProsusAI/finbert\"\n",
    "print(f\"Downloading/Loading Tokenizer for {MODEL_NAME_BERT}...\")\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(MODEL_NAME_BERT)\n",
    "print(f\"Downloading/Loading Tokenizer for {MODEL_NAME_FINBERT}...\")\n",
    "tokenizer_finbert = AutoTokenizer.from_pretrained(MODEL_NAME_FINBERT)\n",
    "\n",
    "\n",
    "\n",
    "# here we tokenize the text data for the BERT model. \n",
    "# We set truncation to True to ensure that texts \n",
    "# longer than the model's maximum input length are truncated, \n",
    "# and we set padding to \"max_length\" to pad shorter texts to the maximum length.\n",
    "# The max_length is set to 128, which is a common choice for BERT-based models.\n",
    "def tokenize_function_bert(examples):\n",
    "    return tokenizer_bert(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "def tokenize_function_finbert(examples):\n",
    "    return tokenizer_finbert(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "# here we map the tokenization on all the threee datasets\n",
    "tokenized_train_bert = train_dataset.map(tokenize_function_bert, batched=True)\n",
    "tokenized_val_bert = val_dataset.map(tokenize_function_bert, batched=True)\n",
    "tokenized_test_bert = test_dataset.map(tokenize_function_bert, batched=True)\n",
    "\n",
    "tokenized_train_finbert = train_dataset.map(tokenize_function_finbert, batched=True)\n",
    "tokenized_val_finbert = val_dataset.map(tokenize_function_finbert, batched=True)\n",
    "tokenized_test_finbert = test_dataset.map(tokenize_function_finbert, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0e865",
   "metadata": {},
   "source": [
    "## Metrics for the modesl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    macro_f1= f1_score(labels, predictions, average='macro')\n",
    "    per_class_f1= f1_score(labels, predictions, average=None)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'macro_f1': macro_f1,\n",
    "        'fq1_|negative': per_class_f1[0],\n",
    "        'fq1_|neutral': per_class_f1[1],\n",
    "        'fq1_|positive': per_class_f1[2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f24bffc",
   "metadata": {},
   "source": [
    "## Load Models and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc27f5b",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd766596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/Loading Model bert-base-uncased...\n",
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/Mehr/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/Mehr/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/transformers/training_args.py:2179: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of  Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BERT fine-tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2694f8170d6246a186889dcda0bc598a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/726 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     37\u001b[39m trainer_bert = Trainer(\n\u001b[32m     38\u001b[39m     model=model_bert,\n\u001b[32m     39\u001b[39m     args=training_args_bert,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m     43\u001b[39m )\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting BERT fine-tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43mtrainer_bert\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating on Test Set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m test_results_bert = trainer_bert.predict(tokenized_test_bert)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/transformers/trainer.py:1948\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1946\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1947\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1948\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1949\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/transformers/trainer.py:2335\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2330\u001b[39m     _grad_norm = nn.utils.clip_grad_norm_(\n\u001b[32m   2331\u001b[39m         amp.master_params(\u001b[38;5;28mself\u001b[39m.optimizer),\n\u001b[32m   2332\u001b[39m         args.max_grad_norm,\n\u001b[32m   2333\u001b[39m     )\n\u001b[32m   2334\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2335\u001b[39m     _grad_norm = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2337\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2338\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2341\u001b[39m     is_accelerate_available()\n\u001b[32m   2342\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED\n\u001b[32m   2343\u001b[39m ):\n\u001b[32m   2344\u001b[39m     grad_norm = model.get_global_grad_norm()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2304\u001b[39m, in \u001b[36mAccelerator.clip_grad_norm_\u001b[39m\u001b[34m(self, parameters, max_norm, norm_type)\u001b[39m\n\u001b[32m   2302\u001b[39m             acc_opt.gradient_state.is_xla_gradients_synced = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2303\u001b[39m \u001b[38;5;28mself\u001b[39m.unscale_gradients()\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/code/26/Mar/assignment/.venv/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:82\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m     80\u001b[39m         clip_coef_clamped_device = clip_coef_clamped.to(device)\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grads:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m             \u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_coef_clamped_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(f\"Downloading/Loading Model {MODEL_NAME_BERT}...\")\n",
    "\n",
    "# Check for CUDA (GPU), then MPS, then default to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# num_layer=3 is used to fine-tune only the last 3 layers of the BERT model.\n",
    "model_bert=AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME_BERT, \n",
    "    num_labels=3, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    "    )\n",
    "model_bert.to(device)\n",
    "\n",
    "training_args_bert = TrainingArguments(\n",
    "    output_dir=\"./bert_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    # use_mps_device is specifically for MPS, Trainer handles CUDA automatically if the model is on cuda\n",
    "    use_mps_device=True if device.type == \"mps\" else False \n",
    ")\n",
    "trainer_bert = Trainer(\n",
    "    model=model_bert,\n",
    "    args=training_args_bert,\n",
    "    train_dataset=tokenized_train_bert,\n",
    "    eval_dataset=tokenized_val_bert,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting BERT fine-tuning...\")\n",
    "\n",
    "trainer_bert.train()\n",
    "print(\"\\nEvaluating on Test Set...\")\n",
    "\n",
    "\n",
    "test_results_bert = trainer_bert.predict(tokenized_test_bert)\n",
    "predictions_bert = np.argmax(test_results_bert.predictions, axis=-1)\n",
    "test_labels_bert = tokenized_test_bert[\"label\"]\n",
    "\n",
    "print(\"\\nClassification Report (Contains Per-Class F1 and Macro F1):\")\n",
    "print(classification_report(test_labels_bert, predictions_bert, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "# Plot\n",
    "cm=confusion_matrix(test_labels_bert, predictions_bert)\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
    "plt.title('Confusion Matrix for BERT Base Uncased')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "save_directory_bert=f\"./fine_tuned_{MODEL_NAME_BERT.replace('/', '_')}\"\n",
    "print(f\"Saving model to {save_directory_bert}...\")\n",
    "\n",
    "trainer_bert.save_model(save_directory_bert)\n",
    "tokenizer_bert.save_pretrained(save_directory_bert)\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f1ced",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Loading the saved model\n",
    "my_model = AutoModelForSequenceClassification.from_pretrained(\"./my_fine_tuned_bert-base-uncased\")\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(\"./my_fine_tuned_bert-base-uncased\")\n",
    "\n",
    "# Pipeline\n",
    "from transformers import pipeline\n",
    "my_classifier = pipeline(\"text-classification\", model=my_model, tokenizer=my_tokenizer)\n",
    "\n",
    "print(my_classifier(\"I got a NVDIA's share three months ago but now i am not happy about it. hahaha\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25656d1",
   "metadata": {},
   "source": [
    "## FinBert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(f\"Downloading/Loading Model {MODEL_NAME_FINBERT}...\")\n",
    "\n",
    "# Check for CUDA (GPU), then MPS, then default to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_finbert=AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME_FINBERT, \n",
    "    num_labels=3, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    "    )\n",
    "model_finbert.to(device)\n",
    "\n",
    "training_args_finbert = TrainingArguments(\n",
    "    output_dir=\"./finbert_finetuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    use_mps_device=True if device.type == \"mps\" else False \n",
    ")\n",
    "trainer_finbert = Trainer(\n",
    "    model=model_finbert,\n",
    "    args=training_args_finbert,\n",
    "    train_dataset=tokenized_train_finbert,\n",
    "    eval_dataset=tokenized_val_finbert,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting FinBERT fine-tuning...\")\n",
    "\n",
    "trainer_finbert.train()\n",
    "print(\"\\nEvaluating on Test Set...\")\n",
    "\n",
    "\n",
    "test_results_finbert = trainer_finbert.predict(tokenized_test_finbert)\n",
    "predictions_finbert = np.argmax(test_results_finbert.predictions, axis=-1)\n",
    "test_labels_finbert = tokenized_test_finbert[\"label\"]\n",
    "\n",
    "print(\"\\nClassification Report (Contains Per-Class F1 and Macro F1):\")\n",
    "print(classification_report(test_labels_finbert, predictions_finbert, target_names=['negative', 'neutral', 'positive']))\n",
    "\n",
    "# Plot\n",
    "cm=confusion_matrix(test_labels_finbert, predictions_finbert)\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])\n",
    "plt.title('Confusion Matrix for Fine-Tuned FinBERT')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "save_directory_finbert=f\"./fine_tuned_{MODEL_NAME_FINBERT.replace('/', '_')}\"\n",
    "print(f\"Saving model to {save_directory_finbert}...\")\n",
    "\n",
    "trainer_finbert.save_model(save_directory_finbert)\n",
    "tokenizer_finbert.save_pretrained(save_directory_finbert)\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d01de5f",
   "metadata": {},
   "source": [
    "## Plots and Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effa7ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1691531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPREHENSIVE MODEL COMPARISON: Before vs After Fine-Tuning\n",
    "# ============================================================\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    f1_score, precision_score, recall_score, \n",
    "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "target_names = ['negative', 'neutral', 'positive']\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. Collect all predictions into a unified structure\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# --- Before Fine-Tuning (baseline pipeline predictions) ---\n",
    "finbert_baseline_preds_int = [label2id[l] for l in finbert_preds_labels]\n",
    "bert_baseline_preds_int = [label2id[l] for l in bert_preds]\n",
    "\n",
    "# Ground truth (int encoded)\n",
    "true_labels_int = [label2id[l] for l in test_labels]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. Helper to compute a full metrics dict\n",
    "# ----------------------------------------------------------\n",
    "def get_all_metrics(y_true, y_pred, model_name, stage):\n",
    "    report = classification_report(y_true, y_pred, target_names=target_names, output_dict=True, zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Stage': stage,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'Weighted F1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'Macro F1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'Micro F1': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        'Weighted Precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'Macro Precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'Weighted Recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'Macro Recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'MCC': matthews_corrcoef(y_true, y_pred),\n",
    "        \"Cohen's Kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "    }\n",
    "    # Per-class metrics\n",
    "    for cls_name in target_names:\n",
    "        metrics[f'Precision ({cls_name})'] = report[cls_name]['precision']\n",
    "        metrics[f'Recall ({cls_name})'] = report[cls_name]['recall']\n",
    "        metrics[f'F1 ({cls_name})'] = report[cls_name]['f1-score']\n",
    "        metrics[f'Support ({cls_name})'] = report[cls_name]['support']\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. Build full comparison table\n",
    "# ----------------------------------------------------------\n",
    "all_metrics = []\n",
    "all_metrics.append(get_all_metrics(true_labels_int, finbert_baseline_preds_int, 'FinBERT', 'Before Fine-Tuning'))\n",
    "all_metrics.append(get_all_metrics(true_labels_int, bert_baseline_preds_int,    'BERT-base-uncased', 'Before Fine-Tuning'))\n",
    "all_metrics.append(get_all_metrics(true_labels_int, predictions_finbert,        'FinBERT', 'After Fine-Tuning'))\n",
    "all_metrics.append(get_all_metrics(true_labels_int, predictions_bert,           'BERT-base-uncased', 'After Fine-Tuning'))\n",
    "\n",
    "full_comparison_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "# Display summary table (key metrics)\n",
    "summary_cols = ['Model', 'Stage', 'Accuracy', 'Balanced Accuracy', \n",
    "                'Weighted F1', 'Macro F1', 'Weighted Precision', 'Weighted Recall',\n",
    "                'MCC', \"Cohen's Kappa\"]\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY COMPARISON TABLE\")\n",
    "print(\"=\" * 80)\n",
    "display(full_comparison_df[summary_cols].round(4))\n",
    "\n",
    "# Display per-class table\n",
    "per_class_cols = ['Model', 'Stage'] + [c for c in full_comparison_df.columns if c.startswith(('Precision (', 'Recall (', 'F1 ('))]\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS METRICS\")\n",
    "print(\"=\" * 80)\n",
    "display(full_comparison_df[per_class_cols].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 1: Side-by-Side Confusion Matrices (All 4 Scenarios)\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "fig.suptitle('Confusion Matrices: Before vs After Fine-Tuning', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "scenarios = [\n",
    "    (true_labels_int, finbert_baseline_preds_int, 'FinBERT (Before Fine-Tuning)', axes[0, 0]),\n",
    "    (true_labels_int, bert_baseline_preds_int,    'BERT-base-uncased (Before Fine-Tuning)', axes[0, 1]),\n",
    "    (true_labels_int, predictions_finbert,        'FinBERT (After Fine-Tuning)', axes[1, 0]),\n",
    "    (true_labels_int, predictions_bert,           'BERT-base-uncased (After Fine-Tuning)', axes[1, 1]),\n",
    "]\n",
    "\n",
    "for y_true, y_pred, title, ax in scenarios:\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=target_names, yticklabels=target_names, ax=ax)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbebea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 2: Grouped Bar Chart - Key Metrics Comparison\n",
    "# ============================================================\n",
    "\n",
    "key_metrics = ['Accuracy', 'Balanced Accuracy', 'Weighted F1', 'Macro F1', 'MCC', \"Cohen's Kappa\"]\n",
    "\n",
    "plot_df = full_comparison_df[['Model', 'Stage'] + key_metrics].copy()\n",
    "plot_df['Label'] = plot_df['Model'] + '\\n(' + plot_df['Stage'].str.replace('Fine-Tuning', 'FT') + ')'\n",
    "\n",
    "x = np.arange(len(key_metrics))\n",
    "width = 0.2\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "for i, (_, row) in enumerate(plot_df.iterrows()):\n",
    "    offset = (i - 1.5) * width\n",
    "    bars = ax.bar(x + offset, [row[m] for m in key_metrics], width, label=row['Label'])\n",
    "    # Add value labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Key Metrics Comparison: Before vs After Fine-Tuning', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(key_metrics, fontsize=11)\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 3: Per-Class F1 Score Comparison (Grouped Bar)\n",
    "# ============================================================\n",
    "\n",
    "f1_cols = [f'F1 ({c})' for c in target_names]\n",
    "plot_df2 = full_comparison_df[['Model', 'Stage'] + f1_cols].copy()\n",
    "plot_df2['Label'] = plot_df2['Model'] + '\\n(' + plot_df2['Stage'].str.replace('Fine-Tuning', 'FT') + ')'\n",
    "\n",
    "x = np.arange(len(target_names))\n",
    "width = 0.2\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "for i, (_, row) in enumerate(plot_df2.iterrows()):\n",
    "    offset = (i - 1.5) * width\n",
    "    bars = ax.bar(x + offset, [row[f'F1 ({c})'] for c in target_names], width, label=row['Label'])\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.set_title('Per-Class F1 Score: Before vs After Fine-Tuning', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(target_names, fontsize=12)\n",
    "ax.set_ylim(0, 1.15)\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05475fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 4: Per-Class Precision & Recall Heatmap\n",
    "# ============================================================\n",
    "\n",
    "prec_recall_cols = ['Model', 'Stage'] + \\\n",
    "    [f'Precision ({c})' for c in target_names] + \\\n",
    "    [f'Recall ({c})' for c in target_names]\n",
    "\n",
    "heatmap_df = full_comparison_df[prec_recall_cols].copy()\n",
    "heatmap_df['Label'] = heatmap_df['Model'] + ' (' + heatmap_df['Stage'].str.replace('Fine-Tuning', 'FT') + ')'\n",
    "heatmap_df = heatmap_df.set_index('Label').drop(columns=['Model', 'Stage'])\n",
    "\n",
    "# Rename columns for cleaner display\n",
    "rename_map = {}\n",
    "for c in target_names:\n",
    "    rename_map[f'Precision ({c})'] = f'Prec-{c[:3].capitalize()}'\n",
    "    rename_map[f'Recall ({c})'] = f'Rec-{c[:3].capitalize()}'\n",
    "heatmap_df = heatmap_df.rename(columns=rename_map)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "sns.heatmap(heatmap_df.astype(float).round(4), annot=True, fmt='.4f', cmap='YlOrRd', \n",
    "            linewidths=0.5, ax=ax, vmin=0, vmax=1)\n",
    "ax.set_title('Per-Class Precision & Recall Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=10, rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ce8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 5: Improvement (Delta) Chart - Before vs After\n",
    "# ============================================================\n",
    "\n",
    "delta_metrics = ['Accuracy', 'Balanced Accuracy', 'Weighted F1', 'Macro F1', 'MCC', \"Cohen's Kappa\"]\n",
    "\n",
    "# Calculate deltas\n",
    "before_finbert = full_comparison_df[(full_comparison_df['Model'] == 'FinBERT') & (full_comparison_df['Stage'] == 'Before Fine-Tuning')].iloc[0]\n",
    "after_finbert  = full_comparison_df[(full_comparison_df['Model'] == 'FinBERT') & (full_comparison_df['Stage'] == 'After Fine-Tuning')].iloc[0]\n",
    "before_bert    = full_comparison_df[(full_comparison_df['Model'] == 'BERT-base-uncased') & (full_comparison_df['Stage'] == 'Before Fine-Tuning')].iloc[0]\n",
    "after_bert     = full_comparison_df[(full_comparison_df['Model'] == 'BERT-base-uncased') & (full_comparison_df['Stage'] == 'After Fine-Tuning')].iloc[0]\n",
    "\n",
    "delta_finbert = [(after_finbert[m] - before_finbert[m]) for m in delta_metrics]\n",
    "delta_bert    = [(after_bert[m] - before_bert[m]) for m in delta_metrics]\n",
    "\n",
    "x = np.arange(len(delta_metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "bars1 = ax.bar(x - width/2, delta_finbert, width, label='FinBERT', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, delta_bert,    width, label='BERT-base-uncased', color='coral')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        sign = '+' if height >= 0 else ''\n",
    "        ax.annotate(f'{sign}{height:.4f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                    xytext=(0, 3 if height >= 0 else -12), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom' if height >= 0 else 'top', fontsize=8)\n",
    "\n",
    "ax.axhline(y=0, color='black', linewidth=0.8, linestyle='-')\n",
    "ax.set_ylabel('Change (After - Before)', fontsize=12)\n",
    "ax.set_title('Metric Improvement After Fine-Tuning', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(delta_metrics, fontsize=11)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0851b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 6: Training Loss Curves (from Trainer logs)\n",
    "# ============================================================\n",
    "\n",
    "def extract_training_logs(trainer, model_name):\n",
    "    \"\"\"Extract training and validation loss/metrics from trainer log history.\"\"\"\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_f1 = []\n",
    "    epochs = []\n",
    "    \n",
    "    for entry in trainer.state.log_history:\n",
    "        if 'loss' in entry and 'eval_loss' not in entry:\n",
    "            train_loss.append(entry['loss'])\n",
    "        if 'eval_loss' in entry:\n",
    "            val_loss.append(entry['eval_loss'])\n",
    "            val_acc.append(entry.get('eval_accuracy', None))\n",
    "            val_f1.append(entry.get('eval_macro_f1', None))\n",
    "            epochs.append(entry.get('epoch', len(epochs)+1))\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'val_f1': val_f1,\n",
    "        'epochs': epochs\n",
    "    }\n",
    "\n",
    "bert_logs = extract_training_logs(trainer_bert, 'BERT-base-uncased')\n",
    "finbert_logs = extract_training_logs(trainer_finbert, 'FinBERT')\n",
    "\n",
    "# --- Plot Training & Validation Loss ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Training Loss\n",
    "for logs, color in [(bert_logs, 'coral'), (finbert_logs, 'steelblue')]:\n",
    "    if logs['train_loss']:\n",
    "        axes[0].plot(range(1, len(logs['train_loss'])+1), logs['train_loss'], \n",
    "                     marker='o', label=f\"{logs['model']} Train Loss\", color=color, linestyle='-')\n",
    "    if logs['val_loss']:\n",
    "        axes[0].plot(logs['epochs'], logs['val_loss'], \n",
    "                     marker='s', label=f\"{logs['model']} Val Loss\", color=color, linestyle='--')\n",
    "\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training & Validation Loss', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "for logs, color in [(bert_logs, 'coral'), (finbert_logs, 'steelblue')]:\n",
    "    if logs['val_acc'] and all(v is not None for v in logs['val_acc']):\n",
    "        axes[1].plot(logs['epochs'], logs['val_acc'], marker='o', label=logs['model'], color=color)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Validation Accuracy per Epoch', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Validation Macro F1\n",
    "for logs, color in [(bert_logs, 'coral'), (finbert_logs, 'steelblue')]:\n",
    "    if logs['val_f1'] and all(v is not None for v in logs['val_f1']):\n",
    "        axes[2].plot(logs['epochs'], logs['val_f1'], marker='o', label=logs['model'], color=color)\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('Macro F1', fontsize=12)\n",
    "axes[2].set_title('Validation Macro F1 per Epoch', fontsize=13, fontweight='bold')\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Dynamics', fontsize=15, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 7: Normalised Confusion Matrices (Percentages)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "class_labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "configs = [\n",
    "    (\"BERT-base Before FT\", bert_preds,           test_labels, axes[0, 0]),\n",
    "    (\"BERT-base After FT\",  predictions_bert,      test_labels, axes[0, 1]),\n",
    "    (\"FinBERT Before FT\",   finbert_preds_labels,  test_labels, axes[1, 0]),\n",
    "    (\"FinBERT After FT\",    predictions_finbert,    test_labels, axes[1, 1]),\n",
    "]\n",
    "\n",
    "for title, preds, labels, ax in configs:\n",
    "    cm = confusion_matrix(labels, preds, labels=class_labels)\n",
    "    cm_pct = cm.astype('float') / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "    annot = np.array([[f\"{cm[i][j]}\\n({cm_pct[i][j]:.1f}%)\" \n",
    "                        for j in range(len(class_labels))] \n",
    "                       for i in range(len(class_labels))])\n",
    "\n",
    "    sns.heatmap(cm_pct, annot=annot, fmt='', cmap='YlOrRd', ax=ax,\n",
    "                xticklabels=class_labels, yticklabels=class_labels,\n",
    "                vmin=0, vmax=100, cbar_kws={'label': '%'})\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted', fontsize=11)\n",
    "    ax.set_ylabel('Actual', fontsize=11)\n",
    "\n",
    "plt.suptitle('Normalised Confusion Matrices (Count & Percentage)',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c803fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 8: Radar / Spider Chart  Before vs After for Each Model\n",
    "# ============================================================\n",
    "\n",
    "from math import pi\n",
    "\n",
    "radar_metrics = ['Accuracy', 'Balanced Accuracy', 'Weighted F1',\n",
    "                 'Macro F1', 'MCC', \"Cohen's Kappa\"]\n",
    "\n",
    "def make_radar(ax, title, row_before, row_after, color_before, color_after):\n",
    "    \"\"\"Draw radar chart comparing before/after on selected metrics.\"\"\"\n",
    "    N = len(radar_metrics)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]  # close the polygon\n",
    "    \n",
    "    vals_before = [row_before[m] for m in radar_metrics] + [row_before[radar_metrics[0]]]\n",
    "    vals_after  = [row_after[m]  for m in radar_metrics] + [row_after[radar_metrics[0]]]\n",
    "    \n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_rlabel_position(0)\n",
    "    \n",
    "    ax.plot(angles, vals_before, 'o-', linewidth=2, label='Before FT', color=color_before)\n",
    "    ax.fill(angles, vals_before, alpha=0.15, color=color_before)\n",
    "    ax.plot(angles, vals_after, 'o-', linewidth=2, label='After FT', color=color_after)\n",
    "    ax.fill(angles, vals_after, alpha=0.15, color=color_after)\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(radar_metrics, fontsize=9)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=9)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Get rows from full_comparison_df\n",
    "bert_before = full_comparison_df[full_comparison_df['Model/Stage'] == 'BERT-base Before FT'].iloc[0]\n",
    "bert_after  = full_comparison_df[full_comparison_df['Model/Stage'] == 'BERT-base After FT'].iloc[0]\n",
    "fb_before   = full_comparison_df[full_comparison_df['Model/Stage'] == 'FinBERT Before FT'].iloc[0]\n",
    "fb_after    = full_comparison_df[full_comparison_df['Model/Stage'] == 'FinBERT After FT'].iloc[0]\n",
    "\n",
    "make_radar(axes[0], 'BERT-base-uncased', bert_before, bert_after, 'coral', 'firebrick')\n",
    "make_radar(axes[1], 'FinBERT',           fb_before,   fb_after,   'steelblue', 'darkblue')\n",
    "\n",
    "plt.suptitle('Radar Chart  Metric Profile Before vs After Fine-Tuning',\n",
    "             fontsize=15, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9511302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PLOT 9: Class Distribution in Test Set + Prediction Distribution\n",
    "# ============================================================\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "class_labels = ['negative', 'neutral', 'positive']\n",
    "palette = {'negative': '#e74c3c', 'neutral': '#95a5a6', 'positive': '#2ecc71'}\n",
    "\n",
    "# Actual distribution\n",
    "actual_counts = Counter(test_labels)\n",
    "axes[0].bar(class_labels, [actual_counts[c] for c in class_labels],\n",
    "            color=[palette[c] for c in class_labels], edgecolor='black')\n",
    "for i, c in enumerate(class_labels):\n",
    "    axes[0].text(i, actual_counts[c]+5, str(actual_counts[c]), ha='center', fontweight='bold')\n",
    "axes[0].set_title('Actual Test Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Prediction distributions  Before FT\n",
    "configs_before = {\n",
    "    'BERT-base': bert_preds,\n",
    "    'FinBERT': finbert_preds_labels\n",
    "}\n",
    "x = np.arange(len(class_labels))\n",
    "width = 0.35\n",
    "for i, (name, preds) in enumerate(configs_before.items()):\n",
    "    counts = Counter(preds)\n",
    "    axes[1].bar(x + i*width, [counts.get(c, 0) for c in class_labels],\n",
    "                width, label=name, edgecolor='black')\n",
    "axes[1].set_xticks(x + width/2)\n",
    "axes[1].set_xticklabels(class_labels)\n",
    "axes[1].set_title('Predicted Distribution  Before FT', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend()\n",
    "\n",
    "# Prediction distributions  After FT\n",
    "configs_after = {\n",
    "    'BERT-base': predictions_bert,\n",
    "    'FinBERT': predictions_finbert\n",
    "}\n",
    "for i, (name, preds) in enumerate(configs_after.items()):\n",
    "    counts = Counter(preds)\n",
    "    axes[2].bar(x + i*width, [counts.get(c, 0) for c in class_labels],\n",
    "                width, label=name, edgecolor='black')\n",
    "axes[2].set_xticks(x + width/2)\n",
    "axes[2].set_xticklabels(class_labels)\n",
    "axes[2].set_title('Predicted Distribution  After FT', fontsize=13, fontweight='bold')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle('Class Distribution: Actual vs Predicted', fontsize=15, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15025bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SUMMARY TABLE  Export-ready for LaTeX / Paper\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON  SUMMARY TABLE\")\n",
    "print(\"=\" * 80)\n",
    "display(full_comparison_df.style\n",
    "        .format(precision=4)\n",
    "        .highlight_max(axis=0, subset=full_comparison_df.select_dtypes(include='number').columns,\n",
    "                       props='background-color: #d4edda; font-weight: bold')\n",
    "        .highlight_min(axis=0, subset=full_comparison_df.select_dtypes(include='number').columns,\n",
    "                       props='background-color: #f8d7da')\n",
    "        .set_caption(\"Model Performance Comparison  Before & After Fine-Tuning\")\n",
    ")\n",
    "\n",
    "# Export to CSV for paper\n",
    "full_comparison_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"\\n Results exported to model_comparison_results.csv\")\n",
    "\n",
    "# --- LaTeX table ---\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LATEX TABLE (copy-paste into paper)\")\n",
    "print(\"=\" * 80)\n",
    "print(full_comparison_df.to_latex(index=False, float_format=\"%.4f\", caption=\"Model Performance Comparison\", label=\"tab:model_comparison\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
